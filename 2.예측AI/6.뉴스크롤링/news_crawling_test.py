# news_crawling_test.py - ì‹¤ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
from news import NewsDataCrolling
import os
from dotenv import load_dotenv

print("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
print("=" * 50)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ í™•ì¸
client_id = os.getenv('Client_ID')
client_secret = os.getenv('Client_Secret')

print("1. API í‚¤ í™•ì¸...")
if client_id and client_secret:
    print(f"   âœ… Client ID: {client_id[:10]}...")
    print(f"   âœ… Client Secret: {client_secret[:10]}...")
else:
    print("   âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    print("   ğŸ’¡ .env íŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•˜ì„¸ìš”:")
    print("      Client_ID=your_naver_client_id")
    print("      Client_Secret=your_naver_client_secret")
    exit()

print("\n2. ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")

# ì‹¤ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œë„
try:
    print("   ğŸ”„ 'ê¸ˆìœµ' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ 5ê°œ ìˆ˜ì§‘ ì¤‘...")
    crawler = NewsDataCrolling("ê¸ˆìœµ", 5)
    news_list = crawler.news_data
    
    if news_list:
        print(f"   âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ!")
        
        print("\nğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡:")
        print("-" * 80)
        for i, news in enumerate(news_list, 1):
            print(f"{i}. ì œëª©: {news['title']}")
            print(f"   ì„¤ëª…: {news['description']}")
            print(f"   ë‚ ì§œ: {news['pubDate']}")
            print(f"   ë§í¬: {news['link']}")
            print("-" * 80)
        
        # ê¸ˆìœµ ë‰´ìŠ¤ í•„í„°ë§ í…ŒìŠ¤íŠ¸
        print("\n3. ê¸ˆìœµ ë‰´ìŠ¤ í•„í„°ë§ í…ŒìŠ¤íŠ¸...")
        filtered_news = NewsDataCrolling.filter_weather_news(news_list)
        
        if filtered_news:
            print(f"   âœ… ê¸ˆìœµ ê´€ë ¨ ë‰´ìŠ¤ {len(filtered_news)}ê°œ í•„í„°ë§ë¨!")
            
            print("\nğŸ¯ í•„í„°ë§ëœ ê¸ˆìœµ ë‰´ìŠ¤:")
            print("-" * 80)
            for i, news in enumerate(filtered_news, 1):
                print(f"{i}. ì œëª©: {news['title']}")
                print(f"   ì¹´í…Œê³ ë¦¬: {news['category']}")
                print(f"   ì¶œì²˜: {news['source']}")
                print(f"   ë‚ ì§œ: {news['pubDate']}")
                print("-" * 80)
        else:
            print("   âš ï¸ ê¸ˆìœµ ê´€ë ¨ ë‰´ìŠ¤ê°€ í•„í„°ë§ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ğŸ’¡ í‚¤ì›Œë“œë¥¼ 'ê²½ì œ' ë˜ëŠ” 'ì£¼ì‹'ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
    
    else:
        print("   âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨!")
        print("   ğŸ’¡ API í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”.")

except Exception as e:
    print(f"   âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()

# ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œë„ í…ŒìŠ¤íŠ¸
print("\n4. ë‹¤ë¥¸ í‚¤ì›Œë“œ í…ŒìŠ¤íŠ¸...")
test_keywords = ["ê²½ì œ", "ì£¼ì‹", "ë¶€ë™ì‚°"]

for keyword in test_keywords:
    try:
        print(f"\n   ğŸ”„ '{keyword}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ 3ê°œ ìˆ˜ì§‘ ì¤‘...")
        crawler = NewsDataCrolling(keyword, 3)
        news_list = crawler.news_data
        
        if news_list:
            print(f"      âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ë¨")
            # ì²« ë²ˆì§¸ ë‰´ìŠ¤ë§Œ ì¶œë ¥
            first_news = news_list[0]
            print(f"      ğŸ“° ì²« ë²ˆì§¸ ë‰´ìŠ¤: {first_news['title'][:60]}...")
        else:
            print(f"      âŒ '{keyword}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
    except Exception as e:
        print(f"      âŒ '{keyword}' í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

print("\n" + "=" * 50)
print("ğŸ‰ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("ğŸ’¡ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ë‹¤ë©´ Djangoì— í†µí•©í•˜ì„¸ìš”!")
